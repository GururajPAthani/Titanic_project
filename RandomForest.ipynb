{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest: \n",
    "Wisdom of the crowd, uses Ensemble Learning: Aggregating the outputs of the crowd and making prediction\n",
    "however, one more practice. For Best Results: Use different models(e.g. Logistic Regression, Naive Bayes and Decision Tree Classifier etc..).\n",
    "\n",
    "**However, if you dont use differnt models, at least use different data sets for training the same Base Learner - \n",
    "this is called Bagging : Bootstrapping(using data with replacement) and Aggregating(aggregate the result of the base learner)**\n",
    "\n",
    "Why to use it?\n",
    "\n",
    "**1. Reduces variance**: Standalone models can result in high variance. Aggregating base models’ predictions in an ensemble help reduce it.\n",
    "\n",
    "**2. Fast**: Training can happen in parallel across CPU cores and even across different servers.\n",
    "\n",
    "**3. Good for big data**: Bagging doesn’t require an entire training dataset to be stored in memory during model training. You can set the sample size for each bootstrap to a fraction of the overall data, train a base learner, and string these base learners together without ever reading in the entire dataset all at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EDA_RandomForest(rf_table_1) :\n",
    "    ##DropColumns if necessary\n",
    "    rf_table=rf_table_1.drop(['Cabin'],axis=1)\n",
    "    rf_table= rf_table.dropna()\n",
    "    print(\"Random Forest EDA output shape = \\n\",rf_table.isna().sum())\n",
    "    print(\"Random Forest : EDA - Complete \\n\")\n",
    "    return(rf_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureEngineering_RandomForest(df_rf_fe) :\n",
    "    ## Feature Selection (\"Drop/Multipy/Create new features/predictors if needed\")\n",
    "    print(\"Random Forest : Feature Engineering \\n\")\n",
    "\n",
    "    print(\"Selected predictors for transformation\\n\")\n",
    "    #Feature transformation \n",
    "    #Convert to string (change datatype)\n",
    "    df_rf_fe['Pclass']= df_rf_fe['Pclass'].astype('str')  \n",
    "    #Dummy encode the data\n",
    "    df_rf_fe = pd.get_dummies(df_rf_fe,drop_first=True)\n",
    "    print(\"Check if categorical variables are in string or dummy encoded \\n\",df_rf_fe.columns,\"\\n\")\n",
    "\n",
    "    #Feature Selection\n",
    "    #Based on the correlation Matrix\n",
    "    # Fare has significan positive correlation on Survival\n",
    "    # Sex_male and Pclass has significant negative correlation on survival\n",
    "    # So lets select Fare and Sex_Male for our modelling\n",
    "   \n",
    "    #df_rf_fe_selected = df_rf_fe.drop(['Embarked_S'],axis=1)\n",
    "    df_rf_fe_selected = df_rf_fe\n",
    "    print(\"Selected predictors for modelling \\n\")\n",
    "    print(df_rf_fe_selected.info(),\"\\n\")\n",
    "    return(df_rf_fe_selected)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most Important Hyperparameters for Random Forest**\n",
    "\n",
    "1. max_depth       : Speciefies how many levels your tree can have, and ultimately determines how many splits it can make.\n",
    "2. min_samples_leaf: Defines the minimum number of samples for a leaf node. i.e. A split can only occur if it guarantees a minimun number of observations in the resulting nodes.\n",
    "3. max_features    : Controls the randomness, It specifies the number of features that each tree randomly selects during training\n",
    "4. n_estimatiors   :  Specifies the number of trees your model will build in its ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HyperParameterTuning_RandomForest() :\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    rf_cv = RandomForestClassifier(random_state=0)\n",
    "    print('Random Forest : Hyperparameter Tuning ')\n",
    "    param_cv_rf = {}\n",
    "    \n",
    "    param_cv_rf = { 'max_depth'         :[2],\n",
    "                    'min_samples_leaf'  :[5],\n",
    "                    'min_samples_split' :[0.3],\n",
    "                    'max_features'      :[2,3],\n",
    "                    'n_estimators'      :[10]\n",
    "                  }\n",
    "    \n",
    "     \n",
    "  #  param_cv_rf = {}#'max_depth'     :[None],\n",
    "                    #'min_samples_leaf'  :[],\n",
    "                    #'min_samples_split' :[],\n",
    "                    #'max_features'      :[],\n",
    "                    #'n_estimators'      :[]\n",
    "    #              }\n",
    "\n",
    "    scoring  = ( 'accuracy' , 'precision' , 'recall' , 'f1')\n",
    "\n",
    "    rf_result = GridSearchCV(rf_cv,param_cv_rf, scoring=scoring, cv=5,refit ='f1')\n",
    "    return(rf_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_data(df_split_1):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X= df_split_1.drop(['Survived'],axis=1)\n",
    "    y=df_split_1['Survived']\n",
    "    X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,stratify=y,random_state=42)\n",
    "    \n",
    "    return(X_train,X_test,y_train,y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
