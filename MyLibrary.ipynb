{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plot as needed \n",
    "### YOUR CODE HERE ###\n",
    "def CorrelationHeatMap(df0) :\n",
    "    # Plot a correlation heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    heatmap = sns.heatmap(df0.corr(), vmin=-1, vmax=1, annot=True, cmap=sns.color_palette(\"vlag\", as_cmap=True))\n",
    "    heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':14}, pad=12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renamefunction() : \n",
    "  print('Rename Function Called')\n",
    "  '''\n",
    "    df0 = df0.rename(columns={'Work_accident': 'work_accident',\n",
    "                          'average_montly_hours': 'average_monthly_hours',\n",
    "                          'time_spend_company': 'tenure',\n",
    "                          'Department': 'department'})\n",
    "   ''' \n",
    "  return()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_results(model_name:str, model_object, metric:str):\n",
    "    '''\n",
    "    Arguments:\n",
    "    model_name (string): what you want the model to be called in the output table\n",
    "    model_object: a fit GridSearchCV object\n",
    "    metric (string): precision, recall, f1, or accuracy\n",
    "\n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean 'metric' score across all validation folds.\n",
    "    '''\n",
    "\n",
    "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
    "    metric_dict = {'precision': 'mean_test_precision',\n",
    "                 'recall': 'mean_test_recall',\n",
    "                 'f1': 'mean_test_f1',\n",
    "                 'accuracy': 'mean_test_accuracy',\n",
    "                 }\n",
    "\n",
    "    # Get all the results from the CV and put them in a df\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(metric) score\n",
    "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
    "\n",
    "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "\n",
    "    # Create table of results\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                        'precision': [precision],\n",
    "                        'recall': [recall],\n",
    "                        'F1': [f1],\n",
    "                        'accuracy': [accuracy],\n",
    "                        },\n",
    "                       )\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_Tree_save_image (rf_cv) :\n",
    "    from sklearn import tree\n",
    "    fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)\n",
    "    tree.plot_tree(rf_cv.best_estimator_.estimators_[0])\n",
    "    fig.savefig('rf_individualtree.png')\n",
    "\n",
    "def plot_model_Tree(rf_cv,X):\n",
    "    from sklearn import tree\n",
    "    # Plot the tree\n",
    "    plt.figure(figsize=(85,20))\n",
    "    tree.plot_tree(rf_cv.best_estimator_, max_depth=6, fontsize=14, feature_names=X.columns, \n",
    "          class_names={0:'stayed', 1:'left'}, filled=True);\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotTreeImportance(clf,X):\n",
    "    #tree2_importances = pd.DataFrame(clf.best_estimator_.feature_importances_, columns=X.columns)\n",
    "    tree2_importances = pd.DataFrame(clf.best_estimator_.feature_importances_, \n",
    "                                    columns=['gini_importance'], \n",
    "                                    index=X.columns\n",
    "                                    )\n",
    "    tree2_importances = tree2_importances.sort_values(by='gini_importance', ascending=False)\n",
    "\n",
    "    # Only extract the features with importances > 0\n",
    "    tree2_importances = tree2_importances[tree2_importances['gini_importance'] != 0]\n",
    "    tree2_importances\n",
    "\n",
    "\n",
    "\n",
    "def GetFeatureImportance(clf_list,Model_Names_list,X,top_n_features):\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(25, 4))  # Adjust figsize as needed\n",
    "\n",
    "    for i in range(0,len(clf_list),1):\n",
    "        clf = clf_list[i]\n",
    "        Model_Name = Model_Names_list[i]\n",
    "        # Get feature importances\n",
    "        feat_impt = clf.best_estimator_.feature_importances_\n",
    "\n",
    "        # Get indices of top 10 features\n",
    "        ind = np.argpartition(clf.best_estimator_.feature_importances_, -top_n_features)[-top_n_features:]\n",
    "\n",
    "        # Get column labels of top 10 features \n",
    "        feat = X.columns[ind]\n",
    "\n",
    "        # Filter `feat_impt` to consist of top 10 feature importances\n",
    "        feat_impt = feat_impt[ind]\n",
    "\n",
    "        y_df = pd.DataFrame({\"Feature\":feat,\"Importance\":feat_impt})\n",
    "        y_sort_df = y_df.sort_values(\"Importance\")\n",
    "        #fig = plt.figure()\n",
    "        #axes[i] = fig.add_subplot(111)\n",
    "\n",
    "        y_sort_df.plot(ax=axes[i],kind='barh',x=\"Feature\",y=\"Importance\")\n",
    "\n",
    "        axes[i].set_title(f\"Feature Importances {Model_Name}\" , fontsize=10)\n",
    "        axes[i].set_ylabel(\"Feature\")\n",
    "        axes[i].set_xlabel(\"Importance\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(model_name:str, model, X_test_data, y_test_data,best_estimatorUse):\n",
    "    \n",
    "    #Generate a table of test scores.\n",
    "\n",
    "    #In: \n",
    "       # model_name (string):  How you want your model to be named in the output table\n",
    "       # model:                A fit GridSearchCV object\n",
    "       # X_test_data:          numpy array of X_test data\n",
    "       # y_test_data:          numpy array of y_test data\n",
    "\n",
    "    #Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if (best_estimatorUse == True) :\n",
    "        preds = model.best_estimator_.predict(X_test_data)\n",
    "    else :\n",
    "        preds = model.predict(X_test_data)\n",
    "\n",
    "\n",
    "    auc = roc_auc_score(y_test_data, preds)\n",
    "    accuracy = accuracy_score(y_test_data, preds)\n",
    "    precision = precision_score(y_test_data, preds)\n",
    "    recall = recall_score(y_test_data, preds)\n",
    "    f1 = f1_score(y_test_data, preds)\n",
    "\n",
    "    table = pd.DataFrame({'model': [model_name],\n",
    "                          'precision': [precision], \n",
    "                          'recall': [recall],\n",
    "                          'f1': [f1],\n",
    "                          'accuracy': [accuracy],\n",
    "                          'AUC': [auc]\n",
    "                         })\n",
    "\n",
    "    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "    cm=confusion_matrix(y_test_data,preds)\n",
    "    #plot_confusion_matrix(cm,model_name)\n",
    "\n",
    "    \n",
    "    #disp=ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=model.classes_)\n",
    "    #disp.plot(values_format='') # `values_format=''` suppresses scientific notation\n",
    "    #plt.show()\n",
    "    '''\n",
    "    title = model_name\n",
    "\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    '''  \n",
    "    return table,cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path to the folder where you want to save the model\n",
    "import pickle\n",
    "def write_pickle(path, model_object, save_name:str):\n",
    "    '''\n",
    "    save_name is a string.\n",
    "    '''\n",
    "    with open(path + save_name + '.pickle', 'wb') as to_write:\n",
    "        pickle.dump(model_object, to_write)\n",
    "        \n",
    "\n",
    "def read_pickle(path, saved_model_name:str):\n",
    "    '''\n",
    "    saved_model_name is a string.\n",
    "    '''\n",
    "    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n",
    "        model = pickle.load(to_read)\n",
    "\n",
    "        return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Impute_upper_limit(column_list,iqr_factor,Q1_Value,Q3_value) :\n",
    "    iqr_value = Q3_value-Q1_Value\n",
    "    print(\"iqr_value = \",iqr_value)\n",
    "    upper_limit = iqr_factor*iqr_value + Q3_value\n",
    "    print(\"upper_limit = \",upper_limit)\n",
    "    for i in range(len(column_list) ):\n",
    "        #print(column_list)\n",
    "        if (column_list[i] > upper_limit) :\n",
    "            column_list[i] = upper_limit     \n",
    "         \n",
    "   # column_list = column_list.mask(column_list>upper_limit,upper_limit)\n",
    "   # print(\"column_list = \",column_list)\n",
    "    return(column_list)\n",
    "\n",
    "def Impute_lower_limit(column_list,iqr_factor,Q1_Value,Q3_value) :\n",
    "    iqr_value = Q3_value-Q1_Value\n",
    "    print(\"iqr_value = \",iqr_value)\n",
    "    lower_limit = Q1_Value-iqr_factor*iqr_value \n",
    "    print(\"lower_limit = \",lower_limit)\n",
    "    for i in range(len(column_list) ):\n",
    "        #print(column_list)\n",
    "        if (column_list[i] < lower_limit) :\n",
    "            column_list[i] = lower_limit     \n",
    "         \n",
    "   # column_list = column_list.mask(column_list>upper_limit,upper_limit)\n",
    "   # print(\"column_list = \",column_list)\n",
    "    return(column_list)\n",
    "\n",
    "def Impute_lower_limit_to_threshold(column_list,threshold_value) :\n",
    "\n",
    "    for i in range(len(column_list) ):\n",
    "        #print(column_list)\n",
    "        if (column_list[i] < threshold_value) :\n",
    "            column_list[i] = threshold_value     \n",
    "         \n",
    "   # column_list = column_list.mask(column_list>upper_limit,upper_limit)\n",
    "   # print(\"column_list = \",column_list)\n",
    "    return(column_list)\n",
    "\n",
    "\n",
    "def Impute_missing_values(column_list,impute_value) :\n",
    "    import math\n",
    "    print(\"impute missiong values by = \",impute_value)\n",
    "\n",
    "    for i in range(len(column_list) ):\n",
    "        #print(column_list)\n",
    "        if (np.isnan(column_list[i])==True) :\n",
    "            column_list[i] = impute_value     \n",
    "         \n",
    "   # column_list = column_list.mask(column_list>upper_limit,upper_limit)\n",
    "   # print(\"column_list = \",column_list)\n",
    "    return(column_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
